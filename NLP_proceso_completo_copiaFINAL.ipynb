{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRABAJO FINAL: PROCESAMIENTO DEL LENGUAJE NATURAL\n",
    "\n",
    "## *FOOD & FEEL: Análisis de sentimientos sobre comentarios de recetas de cocina*\n",
    "### Año 2017\n",
    "### Integrantes\n",
    " * Franco, Laura Sol\n",
    " * Marrero Pizarro, Andrés Maximiliano\n",
    " * Timpani, Pamela Denise\n",
    " * Zamit, Ana Laura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivo del trabajo\n",
    "Se implementará un clasificador que permita realizar un análisis de sentimientos a partir de los comentarios realizados por los usuarios de Facebook en publicaciones realizadas por páginas de cocina que contengan recetas.\n",
    "### Abordaje del desarrollo e implementación del clasificador \n",
    "En los pasos que se irán detallando a continuación se indicará cómo se construye el clasificador, con qué bases de datos se cuenta, como se realiza la tokenización de las palabras, limpieza y acondicionamiento de los datos con los que se cuenta, desarrollo del clasificador del tipo no suvervisado en Python.\n",
    "#### Importación de librerías\n",
    "Las librerías de Python que se utilizarán para la labor son las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuración de la API de Facebook\n",
    "A continuación se destacan los parámetros utilizados para acceder a las páginas de Facebook de interés como *Upsoclsabores* o *Cucinareok* y tomar los comentarios de los usuarios.\n",
    "\n",
    "El usuario y la password utilizada junto con el token de acceso, los id de las páginas son:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "claves = list(line.strip() for line in open('C:/Users/Kikes/Documents/maestria/NLP/app_clave.txt'))\n",
    "\n",
    "app_id = claves[0]\n",
    "\n",
    "app_secret = claves[1] # DO NOT SHARE WITH ANYONE!\n",
    "\n",
    "access_token = app_id + \"|\" + app_secret\n",
    "\n",
    "\n",
    "#recetasdecocinatradicional\n",
    "#recetasdecocinasabrosas\n",
    "#recetasdecocinaclasica1\n",
    "#tastyrecetasdecocina\n",
    "\n",
    "#page_id = 'nytimes'\n",
    "#page_id = 'www.salesintacc.com.ar'\n",
    "#page_id = 'upsoclsabores'\n",
    "#page_id = 'recetasdecocinatradicional'\n",
    "#page_id = 'recetasdecocinasabrosas'\n",
    "#page_id = 'recetasdecocinaclasica1'\n",
    "#page_id = 'tastyrecetasdecocina'\n",
    "page_id = 'cucinareok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acceso a Facebook\n",
    "Luego, se procede a generar un permiso para ingresar a la aplicación donde, en caso de error, se reinicia la conexión a la misma hasta que se garantice el acceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def request_until_succeed(url):\n",
    "    http = urllib3.PoolManager()\n",
    "    req = http.request('GET', url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try:\n",
    "            response = http.urlopen('GET',url)\n",
    "            if response.status == 200:\n",
    "                success = True\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "            \n",
    "            print (\"Error for URL %s: %s\" % (url, datetime.datetime.now()))\n",
    "\n",
    "    return req"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de la función de recopilación\n",
    "En el siguiente fragmento del proceso se define cómo será la función que ingresará en la página de Facebook seleccionada. En este caso, tal como se observa en el apartado de **Configuración de la API de Facebook** se seleccionó de _Cucinare_ cuáles son los parámetros a recoger dentro de los que se destacan: el comentario, el usuario que lo realiza y la cantidad de veces que fue compartida la publicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFacebookPageFeedData(page_id, access_token, num_statuses):\n",
    "    \n",
    "    # construct the URL string\n",
    "    base = \"https://graph.facebook.com/v2.11\" #cambié v2.4 a v2.11, funcionaba igual con v2.4\n",
    "    node = \"/\" + page_id + \"/feed\"\n",
    "    parameters = \"/?fields=message,link,created_time,type,name,id,comments,from,attachment,shares&limit=%s&access_token=%s\" % (num_statuses, access_token)\n",
    "    url = base + node + parameters\n",
    "    \n",
    "    # Esto está cambiado respecto al ejemplo\n",
    "    http = urllib3.PoolManager()\n",
    "    req = http.request('GET', url)\n",
    "    data = json.loads(request_until_succeed(url).data.decode('utf-8'))\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se invoca la función definida en el paso previo junto con los parámetros de la API y se obtienen 100 posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Estoy levantando 100 posts\n",
    "salida_all = getFacebookPageFeedData(page_id, access_token, 100)\n",
    "\n",
    "columns = ['post_from', 'post_id', 'post_name', 'post_type', 'post_message', 'post_link', 'post_shares', 'created_time']\n",
    "\n",
    "df_posts = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede entonces a crear la función que toma la salida de la web y divide los componentes de la misma en las variables que se utilizarán luego para el análisis de sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrapPosts(post):\n",
    "    post_from = '' if 'from' not in post.keys() else post['from']['id']\n",
    "    post_id = post['id']\n",
    "    post_name = '' if 'name' not in post.keys() else post['name']\n",
    "    post_type = '' if 'type' not in post.keys() else post['type']\n",
    "    post_message = '' if 'message' not in post.keys() else post['message']\n",
    "    post_link = '' if 'link' not in post.keys() else post['link']\n",
    "    post_shares = '' if 'shares' not in post.keys() else post['shares']['count']\n",
    "    created_time = datetime.datetime.strptime(post['created_time'],'%Y-%m-%dT%H:%M:%S+0000')\n",
    "    created_time = created_time + datetime.timedelta(hours=-5) # EST\n",
    "    created_time = created_time.strftime('%Y-%m-%d %H:%M:%S')    \n",
    "\n",
    "    return(post_from, post_id, post_name, post_type, post_message, post_link, post_shares, created_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por consiguiente, se aplica la función creada previamente y se crea un dataset compuesto por las variables allí generadas. Adicionalmente, el nuevo dataset es guardado para la posible creación de una base de datos a futuro que sea de utlidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_posts = len(salida_all['data'])\n",
    "\n",
    "for i in range(n_posts):\n",
    "    df = pd.DataFrame(list(scrapPosts(salida_all[\"data\"][i])),index = ['post_from', 'post_id', 'post_name', 'post_type', 'post_message', 'post_link', 'post_shares', 'created_time']).transpose()\n",
    "    df_posts = df_posts.append(df, ignore_index=True)    \n",
    "\n",
    "df_posts.to_csv( 'df_posts'+page_id+'.csv', encode = 'utf-8' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tratamiento de los comentarios\n",
    "Se crea la función que procesa los comentarios en cada post.  Se \"expanden\" los comentarios creando un registro por cada uno. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrapComments(pos, com):\n",
    "\n",
    "    post_id = salida_all[\"data\"][pos][\"id\"]\n",
    "    comm_id = salida_all[\"data\"][pos][\"comments\"][\"data\"][com][\"id\"]\n",
    "    comm_msg = salida_all[\"data\"][pos][\"comments\"][\"data\"][com][\"message\"]\n",
    "    comm_date = datetime.datetime.strptime(salida_all[\"data\"][pos][\"comments\"][\"data\"][com][\"created_time\"],'%Y-%m-%dT%H:%M:%S+0000')\n",
    "    comm_date = comm_date + datetime.timedelta(hours=-5) # EST\n",
    "    comm_date = comm_date.strftime('%Y-%m-%d %H:%M:%S') # best time format for spreadsheet programs\n",
    "    \n",
    "    return(post_id, comm_id, comm_msg, comm_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a crear el dataset que va a guardar los comentarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_comms = pd.DataFrame(columns=['post_id','comm_id','comm_msg','comm_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se ejecuta la función mediante la cual se crea un campo que traduce comentarios en registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#n_posts = len(salida_all['data'])\n",
    "\n",
    "for i in range(n_posts):\n",
    "    if len(salida_all[\"data\"][i]) <= 8:\n",
    "        continue\n",
    "    ncomm = len(salida_all[\"data\"][i][\"comments\"][\"data\"])\n",
    "    for c in range(ncomm):\n",
    "        df = pd.DataFrame(list(scrapComments(i,c)), index=['post_id','comm_id','comm_msg','comm_date']).transpose()\n",
    "        df_comms = df_comms.append(df, ignore_index=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tratamiento de Emojis\n",
    "Una vez creados los registros, se establecen todos los patrones necesarios para extraer y clasificar emojis de los comentarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "df_comms['emojis'] = df_comms['comm_msg'].apply(lambda x: emoji_pattern.findall(x))\n",
    "df_comms['emojis'] = df_comms['emojis'].apply(lambda x: ''.join(x))\n",
    "df_comms['comm_msg'] = df_comms['comm_msg'].apply(lambda x: emoji_pattern.sub(r'', x))\n",
    "df_comms['emojis_count'] = df_comms.emojis.str.len()\n",
    "\n",
    "emoji_pos = '[\\U0001F600|\\U0001F601|\\U0001F602|\\U0001F923|\\U0001F603|\"\\\n",
    "    \"\\U0001F604|\\U0001F605|\\U0001F606|\\U0001F609|\\U0001F60A|\"\\\n",
    "    \"\\U0001F60B|\\U0001F60E|\\U0001F60D|\\U0001F618|\\U0001F617|\"\\\n",
    "    \"\\U0001F619|\\U0001F61A|\\U0001F642|\\U0001F917|\\U0001F929]+'\n",
    "\n",
    "\n",
    "emoji_neg = '[\\U0001F641|\\U0001F616|\\U0001F61E|\\U0001F61F|\\U0001F624|\\U0001F92C|\"\\\n",
    "    \"\\U0001F622|\\U0001F62D|\\U0001F626|\\U0001F627|\\U0001F628|\"\\\n",
    "    \"\\U0001F629|\\U0001F92F|\\U0001F62C|\\U0001F630|\\U0001F631|\"\\\n",
    "    \"\\U0001F633|\\U0001F92A|\\U0001F635|\\U0001F621|\\U0001F620|]+'\n",
    "\n",
    "df_comms['emojis_pos'] = df_comms['emojis'].apply(lambda x: re.findall(emoji_pos,x))\n",
    "df_comms['emojis_pos'] = df_comms['emojis_pos'].apply(lambda x: ''.join(x))\n",
    "df_comms['emojis_pos_count'] = df_comms.emojis_pos.str.len()\n",
    "\n",
    "df_comms['emojis_neg'] = df_comms['emojis'].apply(lambda x: re.findall(emoji_neg,x))\n",
    "df_comms['emojis_neg'] = df_comms['emojis_neg'].apply(lambda x: ''.join(x))\n",
    "df_comms['emojis_neg_count'] = df_comms.emojis_neg.str.len()\n",
    "\n",
    "cant_neutros = df_comms['emojis_count'] - df_comms['emojis_neg_count'] - df_comms['emojis_pos_count']\n",
    "\n",
    "df_comms['emojis_neutros_count'] = cant_neutros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar con esta sección, procedemos a guardar la tabla de comentarios para su posterior manipulación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_comms.to_csv( 'df_comms_'+page_id+'.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo del proceso de NER tagging & feature engineering\n",
    "El input de esta sección es una copia del dataset original denominado *df_comms.copy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = df_comms.copy().head(10)\n",
    "df = df_comms.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generación de variables y acondicionamiento de la base\n",
    "En primer lugar, se crea la variable **\"solo emojis\"** para determinar los registros que solo presentan ideogramas y se realiza un drop de los que no presentan comentarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['comm_msg'].replace('', np.nan, inplace = True)\n",
    "df['emojis'].replace('', np.nan, inplace = True)\n",
    "df['solo_emoji'] = np.where(df.comm_msg.isnull()&df.emojis.notnull(),1,0)\n",
    "df.dropna( subset = ['comm_msg', 'solo_emoji'], inplace = True, how = 'all' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es de suma importancia referenciar la \"java machine\" dado que esta es utilizada por la librería *Stanford NER Tagger* para luego poder reconocer los nombres propios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "java_path = 'C:/Program Files (x86)/Java/jre1.8.0_131/bin/java.exe'\n",
    "\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "os.chdir('C:/Users/Kikes/Documents/maestria/NLP/')\n",
    "st = StanfordNERTagger('stanford-ner/classifiers/spanish.ancora.distsim.s512.crf.ser.gz',\n",
    "\t\t\t\t\t   'stanford-ner/stanford-ner.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base al punto anterior, se define la función para extraer entidades *\"nombre\"* de los comentarios denominada **\"extraeNombres\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extraeNombres(comm):\n",
    "    comm_str = str(comm)#comm.to_json()\n",
    "    #comm_token = word_tokenize(comm_str)\n",
    "    comm_token = word_tokenize(comm_str)\n",
    "    #comm_token = word_tokenize(comm)\n",
    "    comm_class = st.tag(comm_token)\n",
    "    comm_list_per = [pers for pers in comm_class if pers[1] =='PERS']\n",
    "    comm_per = re.sub(r'[\\[\\]\\(\\)\\'\\,]', \"\", str([i[0] for i in comm_list_per]))\n",
    "    return comm_per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se aplica la función **\"extraeNombres\"** para \"taggear\" nombres, se crea una columna nueva con esas entidades identificadas que luego se quitan del comentario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['nombres'] = df['comm_msg'].apply(lambda x: extraeNombres(x))\n",
    "df['nombres'].replace( '', np.nan, inplace = True )\n",
    "\n",
    "#Guardo el dataset para recuperarlo si se corrompe en pasos posteriores\n",
    "df.to_csv( 'df_con_nombres.csv', encoding = 'utf-8')\n",
    "\n",
    "n_comm = len(df)\n",
    "\n",
    "for c in range(n_comm):\n",
    "    ext_name = str(df.nombres[c])\n",
    "    ext_comm = str(df.comm_msg[c])\n",
    "    comm_no_name = ext_comm.replace(ext_name,\"\")\n",
    "    df.comm_msg[c] = comm_no_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, se implementan nuevas variables que sirvan al algoritmo para clasificar:\n",
    " * __tiene_nombre__: variable categórica que indica 1 cuando el comentario tiene nombre propio o 0 cuando no.\n",
    " * __tiene_emoji__: variable categórica que indica 1 cuando el comentario tiene emojis o 0 cuando no.\n",
    " * __largo_comm__: variable que indica el largo del string (comentario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tiene_nombre'] = np.where( df.nombres.notnull(), 1, 0 )\n",
    "df['tiene_emoji'] = np.where( df.emojis.notnull(), 1, 0 )\n",
    "df['comm_msg'].fillna(' ', inplace=True)\n",
    "df['largo_comm'] = df['comm_msg'].apply(lambda x: len( x.replace(\" \",\"\") ))\n",
    "df['comm_msg'] = df['comm_msg'].apply(lambda x: re.sub(r'[!#$%&\\'()*+,-./:;<=>?@\\[\\]^_`{|}~¡]', \" \", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lista de stopWords\n",
    "Inicialmente se carga de lista con stopWords en español para después generar la función que permita extraerlas del string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('spanish'))\n",
    "sw2 = set(line.strip() for line in open('stop_words_acentos.txt'))\n",
    "stop2 = stop|sw2\n",
    "\n",
    "def quitaStopWords(comm):\n",
    "\n",
    "    split_comm = comm.lower().split()\n",
    "    comm_no_stop = [i for i in split_comm if i not in stop2]\n",
    "    comm_str = ' '.join(comm_no_stop)\n",
    "    comm_str_no_short = ' '.join(word for word in comm_str.split() if len(word)>2)\n",
    "    return comm_str_no_short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por consiguiente, se crean las variables predictivas en base al comentario sin stopWords detaladas a continuación:\n",
    " * __comm_msg_clean__: variable en la cual se eliminan las *stopWords* del comentario y se presenta el mismo sin ellas.\n",
    " * __largo_comm_clean__: variable que indica el largo del string (comentario) sin las *stopWords*.\n",
    " * __ratio_largo_clean__: varible que indica el ratio del largo del string sin las *stopWords* en referencia al valor original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['comm_msg_clean'] = df['comm_msg'].apply( lambda x: quitaStopWords( x ) )\n",
    "df['largo_comm_clean'] = df['comm_msg_clean'].apply(lambda x: len(x.replace(\" \",\"\")))\n",
    "df['ratio_largo_clean'] = df['largo_comm_clean']/df['largo_comm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentimientos positivos y negativos\n",
    "En esta parte del proceso, se incorporan dos variables que se espera contribuyan sustancialmente a la clasificación.  Utilizando una lista de palabras pre-clasificada como positivo y negativo se definen las funciones __posWords__ y __negWords__ respectivamente donde luego se crea un ratio para cada una que representa la proporción de palabras positivas / negativas presentes en cada comentario. Las variables son:\n",
    " * __word_pos_ratio__: variable que indica el ratio de palabras positivas en el comentario.\n",
    " * __word_neg_ratio__: variable que indica el ratio de palabras negativas en el comentario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_posNeg = pd.read_csv('C:/Users/Kikes/Documents/maestria/NLP/palabras_pos_neg.csv', encoding='utf-8', index_col=0)\n",
    "\n",
    "df_pos = df_posNeg[['Palabra','V.Mean.Sum']][df_posNeg['V.Mean.Sum']>=5]\n",
    "df_pos.columns = ['palabra', 'valor']\n",
    "pos_words = set(pd.Series(df_pos.palabra))\n",
    "\n",
    "df_neg = df_posNeg[['Palabra','V.Mean.Sum']][df_posNeg['V.Mean.Sum']<5]\n",
    "df_neg.columns = ['palabra', 'valor']\n",
    "neg_words = set(pd.Series(df_neg.palabra))\n",
    "\n",
    "def posWords(comm):\n",
    "    split_comm = comm.lower().split()\n",
    "    cant_palabras = len(split_comm)\n",
    "    word_pos = [i for i in split_comm if i in pos_words]\n",
    "    word_pos2 = ' '.join(word_pos)\n",
    "    word_pos_count = len(word_pos2.split())\n",
    "    word_pos_ratio = 0 if word_pos_count == 0 else  word_pos_count/cant_palabras \n",
    "    return word_pos_ratio\n",
    "\n",
    "def negWords(comm):\n",
    "    split_comm = comm.lower().split()\n",
    "    cant_palabras = len(split_comm)\n",
    "    word_neg = [i for i in split_comm if i in neg_words]\n",
    "    word_neg2 = ' '.join(word_neg)\n",
    "    word_neg_count = len(word_neg2.split())\n",
    "    word_neg_ratio = 0 if word_neg_count == 0 else  word_neg_count/cant_palabras \n",
    "    return word_neg_ratio\n",
    "\n",
    "df['pos'] = df['comm_msg_clean'].apply( lambda x: posWords( x ) )\n",
    "df['neg'] = df['comm_msg_clean'].apply( lambda x: negWords( x ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El algoritmo K-means\n",
    "Dada la naturaleza del objetivo del trabajo, se determina utilizar el algoritmo *K-means* para desarrollar el clasificador. Por este motivo se crea un nuevo dataset para entrenar con el algoritmo *K-means*, dejando solo variables predictivas numéricas y se exporta.\n",
    "\n",
    "Adicionalmente, se exporta el dataset completo para poder asignarle clusters luego de la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_km = df[df.comm_msg_clean!='']\n",
    "df_km = df[['comm_id','emojis_count','emojis_pos_count','emojis_neg_count','solo_emoji','tiene_nombre','tiene_emoji','largo_comm','largo_comm_clean','ratio_largo_clean','pos','neg']]\n",
    "df_km.to_csv( 'df_para_km.csv', encoding = 'utf-8')\n",
    "df_final = df[df.comm_msg_clean!='']\n",
    "df_final.to_csv( 'df_final.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de clusters:\n",
    "Utilizando el algoritmo *K-means* se crearán 3 clusters que deberían representar:\n",
    "* __Comentarios positivos__\n",
    "* __Comentarios negativos__\n",
    "* __Comentarios neutros__\n",
    "\n",
    "Para ello, se deben normalizar los valores de la matriz input con el objetivo de facilitar y mejorar el trabajo de clasificación del algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del df_km['comm_id']\n",
    "normalized_df = (df_km-df_km.mean())/df_km.std()\n",
    "normalized_df.fillna(value=0, inplace=True)\n",
    "km = KMeans(n_clusters = 3)\n",
    "km.fit(normalized_df)\n",
    "predict=km.predict(normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrega la variable __\"clusters\"__ con la clasificación correspondiente al dataset general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['clusters'] = pd.Series( predict, index = normalized_df.index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_wc = df[['comm_id','comm_msg_clean','clusters']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_wc.to_csv( 'df_wordColud.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado final del análisis\n",
    "A continuación, se representan los clusters y sus palabras más significativas para arribar a la conclusión del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_wc.fillna(value = \"\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster1 = str(df_wc[\"comm_msg_clean\"][df_wc.clusters==0])\n",
    "cluster2 = str(df_wc[\"comm_msg_clean\"][df_wc.clusters==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcloud1 = WordCloud(width=900,height=500, max_words=20,relative_scaling=1).\\\n",
    "generate(cluster1)\n",
    "plt.imshow(wordcloud1, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcloud2 = WordCloud(width=900,height=500, max_words=20,relative_scaling=1).\\\n",
    "generate(cluster2)\n",
    "plt.imshow(wordcloud2, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
